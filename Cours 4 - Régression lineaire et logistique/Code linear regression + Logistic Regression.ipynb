{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code forward pass\n",
    "\n",
    "Note that the way scikit-learn encode the datasets is different (row represents a sample, columns features). So we need to change the matrix order in the multiplication\n",
    "\n",
    "\n",
    "$\\mathbf{x} = \\begin{bmatrix}\n",
    "x_0\\\\ \n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "...\\\\ \n",
    "x_d\n",
    "\\end{bmatrix}$ ,       $\\mathbf{w} = \\begin{bmatrix}\n",
    "w_0\\\\ \n",
    "w_1\\\\ \n",
    "w_2\\\\ \n",
    "...\\\\ \n",
    "w_d\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\large h(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x}$\n",
    "\n",
    "This formula assumes each exemple is represented by a column. However in Python (scikit-learn we do the oposite). \n",
    "\n",
    "Also the weights are represented by row vectors instead of columns (1D numpy arrays)\n",
    "\n",
    "So for implemetation we need to change to:\n",
    "\n",
    "\n",
    "$\\large h(\\mathbf{x}) = \\mathbf{x}\\mathbf{w}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1], [2], [3]])\n",
    "y = np.array([1, 2, 3])\n",
    "w = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(X, W):\n",
    "    return np.dot(X, W)\n",
    "    #return np.matmul(X, W)\n",
    "hx = forward(X, w)\n",
    "hx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code MSE\n",
    "\n",
    "Equation:\n",
    "\n",
    "$\\Large J(\\mathbf{w}) = \\frac{1}{2N}\\sum_{i=1}^{N} (h(\\mathbf{x}_i) - y_i)^2$\n",
    "\n",
    "4 steps:\n",
    "\n",
    "- Get the difference between predictins and targets\n",
    "- Square their differences\n",
    "- Get the mean difference\n",
    "- divide by two\n",
    "\n",
    "Test case:\n",
    "\n",
    "- hx = [0, 0, 0]\n",
    "\n",
    "- y = [1, 2, 3]\n",
    "\n",
    "Result\n",
    "\n",
    "- J(w) = 2.333333...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3333333333333335"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_MSE(hx, y):\n",
    "    n_samples = y.size\n",
    "    diff = (hx - y)\n",
    "    diff_squared = diff ** 2\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "    MSE = sum_diff_squared/(2 * n_samples)        \n",
    "    return MSE\n",
    "compute_MSE(hx, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Gradients computation\n",
    "\n",
    "$\\large \\nabla J(\\mathbf{W}) = \\frac{1}{N} \\sum_{i=1}^{N} (h(\\mathbf{x}_i) - y_i)\\mathbf{x}_i$\n",
    "\n",
    "Test case:\n",
    "\n",
    "- hx = [0, 0, 0]\n",
    "\n",
    "- X = [1, 2, 3]\n",
    "\n",
    "- y = [1, 2, 3]\n",
    "\n",
    "Result:\n",
    "\n",
    "- J'(w) = -4.6666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.66666667])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_gradient(X, hx, y):\n",
    "        gradient = np.dot((hx - y), X)\n",
    "        gradient = gradient/y.size\n",
    "        return gradient\n",
    "grad = compute_gradient(X, hx, y)\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Gradient update\n",
    "\n",
    "$\\mathbf{w}^{iter+1} = \\mathbf{w}^{iter} - \\alpha \\nabla J(\\mathbf{w})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "w = w - (alpha * grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression Linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \n",
    "    def __init__(self, n_iter, learning_rate):\n",
    "        self.n_iter = n_iter # for stopping criteria, we could do better here...\n",
    "        self.learning_rate = learning_rate\n",
    "        self.J_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Function with the training code. Learn the parameters W of the model\"\"\"\n",
    "        \n",
    "        # keep information of number of examples and dimensions.\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        # add ones for the bias.\n",
    "        x_0 = np.ones((self.n_samples, 1))\n",
    "        X_tmp = np.hstack((x_0, X))\n",
    "        \n",
    "        # initialize weights 'W' of the model randomly\n",
    "        self.W = np.random.rand(self.n_features + 1) # +1 include biais\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            # get prediction of current weights. (forward pass)\n",
    "            hx = self.forward(X_tmp)\n",
    "            \n",
    "            # check the cost to know whether the model is learning or not.\n",
    "            J = self.compute_cost(hx, y)\n",
    "            self.J_history.append(J)\n",
    "            \n",
    "            # Gradient descent.\n",
    "            gradient = self.compute_gradient(X_tmp, hx, y)\n",
    "            self.W = self.W - (self.learning_rate * gradient) # update weights.\n",
    "            \n",
    "    def forward(self, X):\n",
    "        hx = np.matmul(X, self.W)\n",
    "        return hx\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"Our hypothesis h(x) to be applied over test data\"\"\"\n",
    "        # add the ones vector.\n",
    "        x_0 = np.ones((X.shape[0], 1))\n",
    "        X_tmp = np.hstack((x_0, X))\n",
    "        \n",
    "        hx = self.forward(X_tmp)\n",
    "        return hx\n",
    "    \n",
    "    def compute_cost(self, h_x, y):\n",
    "        n_samples = y.size\n",
    "        diff_squared = (h_x - y) ** 2\n",
    "        sum_diff_squared = np.sum(diff_squared)\n",
    "        MSE = sum_diff_squared/(2 * n_samples)\n",
    "        return MSE\n",
    "    \n",
    "    def compute_gradient(self, X, h_x, y):\n",
    "        gradient = np.dot((h_x - y), X)\n",
    "        return gradient / y.size\n",
    "    \n",
    "    def plot_learning_curve(self):\n",
    "        plt.plot(np.arange(self.n_iter), self.J_history)\n",
    "        plt.xlabel('Iteration', fontsize=18)\n",
    "        plt.ylabel('J(w)', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression implementation (Classification)\n",
    "# Since it is pretty much the same as Linear, here I use inheritance to\n",
    "# make things easier. Only thing that chages are:\n",
    "\n",
    "# 1) addition of the logistic function after the linear combination\n",
    "# 2) threshold for prediction (0.5)\n",
    "class LogisticRegressionOurs(LinearRegression):\n",
    "    \n",
    "    def forward(self, X):\n",
    "        z = super().forward(X)\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = super().predict(X) >= 0.5\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as LRSklearn\n",
    "\n",
    "logistic_sklearn = LRSklearn()\n",
    "logistic_ours = LogisticRegressionOurs(1000, 0.1)\n",
    "X, y = make_classification(n_samples=1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_ours.fit(X_train, y_train)\n",
    "logistic_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.924\n",
      "0.924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_ours = logistic_ours.predict(X_test)\n",
    "y_pred_sklearn = logistic_sklearn.predict(X_test)\n",
    "print(accuracy_score(y_pred_ours, y_test))\n",
    "print(accuracy_score(y_pred_sklearn, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:log635]",
   "language": "python",
   "name": "conda-env-log635-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
